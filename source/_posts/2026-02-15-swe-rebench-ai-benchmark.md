---
title: 🏆 SWE-rebench 曝光 AI Benchmark 骗局：中国模型集体翻车
date: 2026-02-15 20:48:04
tags:
  - AI
  - Benchmark
  - SWE-rebench
  - Claude
  - 中国AI
categories:
  - 深度思考
---

## 引言：AI 竞赛的真相

 everyone 都认为 AI 竞赛非常激烈，差距很小。

但实际上，**一切都是假象**。

一个名为 **SWE-rebench** 的新 benchmark 彻底揭穿了这场骗局。

---

## 一、Benchmark 的骗局

### 传统 Benchmark 的运作方式

- 创建标准化测试
- 给每个模型测试
- 分数最高的获胜

看似简单，但有一个**致命缺陷**：

**题目是公开的！**

### 如何"作弊"

资源有限的实验室不需要构建更好的模型，只需要：
1. 获取公开的测试题目
2. 针对这些特定题目训练模型
3. 过度拟合（overfit）在这些题目上

**结果**：分数很高，但模型并没有真正变聪明，只是**记住了答案**。

这正是过去 12 个月 AI 行业正在发生的事情。

---

## 二、SWE-rebench：无法作弊的测试

### 什么是 SWE-bench？

SWE-bench 是 AI 编程领域最流行的 benchmark 之一。

每个实验室都在用，每次模型发布都会引用它。它成为衡量 AI 是否能真正解决工程问题的标准。

### 问题：已被完全饱和

- 题目公开太久
- 实验室（尤其是中国实验室）专门针对这些题目设计训练数据
- 不是为了构建更好的模型，而是为了在测试中拿高分

### SWE-rebench 的改革

- 从最近的 GitHub 仓库中提取**全新任务**
- **从没人见过的全新问题**
- 相同难度、相同格式、相同挑战

**唯一区别：你无法作弊。**

---

## 三、残酷的数据

### 测试结果

| 排名 | 模型 | 得分 |
|------|------|------|
| 🥇 1 | Claude Code (Opus 4.6) | **52.9%** |
| 🥈 2 | Claude Opus 4.6 | 51.7% |
| 🥉 3 | GPT-5.2 | ~51% |
| 4 | Sonnet 4.5 | 47.1% |
| 5 | Gemini 3 Pro Preview | 46.7% |
| 6 | Codex | 44.0% |

**Top 10 全部是 Anthropic、OpenAI 和 Google 的模型。**

### 中国模型的表现

| 模型 | SWE-bench | SWE-rebench | 下降 |
|------|-----------|-------------|------|
| Kimi K2 Thinking | - | 43.8% | - |
| GLM-5 | - | 42.1% | - |
| Qwen3-Coder-Next | - | 40.0% | - |
| **MiniMax M2.5** | **80.2%** | **39.6%** | **-40.6%** |
| Kimi K2.5 | - | 37.9% | - |

### 关键对比

- **MiniMax M2.5** 官方宣称：80.2%（与 Opus 4.6 的 80.8% 基本持平）
- 叙事到处都说：中国开源模型已经追平

- **SWE-rebench（全新题目）**：
  - MiniMax M2.5：**39.6%**
  - Opus 4.6：**51.7%**
  - **差距：12 个百分点**

在旧 benchmark 上完全看不到这个差距。

---

## 四、他们在复制，不是在竞争

### 中国的方式

中国一直擅长**复制技术**，而不是发明：
- 智能手机
- 社交媒体
- 电动汽车
- 现在是 AI

### AI 不能只复制架构

但 AI 不一样：
- 需要算力
- 需要人才
- 需要数据基础设施
- 需要能从零构建前沿模型的研究文化

**中国实验室比 Anthropic 和 OpenAI 少：**
- 更少的资金
- 更少的 GPU
- 更少的研究人才

**所以他们作弊。**

### 典型的"高分低能"

- 选择最流行的 benchmark
- 获取公开的题目
- 专门设计训练数据过度拟合

**结果**：模型在排行榜上看似世界级。但一旦给出一个它没有被训练过的问题，就崩溃了。

**这不是创新，这是记忆。**

---

## 五、训练数据才是一切

### 真正的启示

> 这不仅仅是关于分数。这是关于**模型是否真正学会了推理**，还是只是学会了应试。

当题目是新的、没见过的：
- 西方模型仍然表现良好
- 中国模型大幅下滑

**这说明：**
- 西方模型学会了**泛化能力**
- 中国模型只是**记住了特定题目**

### 深远影响

1. **重新评估中国 AI 实力**
   - 真实的 AI 能力可能比宣传的低 40-50%
   
2. **Benchmark 行业地震**
   - 所有基于公开题目的测试都需要改革
   - 需要更多"SWE-rebench"风格的动态测试

3. **研究文化的重要性**
   - 真正的创新来自研究文化，而非应试技巧
   - 中国 AI 需要从"复制"转向"原创"

---

## 六、我的观点

### 关于这篇文章

这篇文章有明显的**立场倾向**——作者是西方AI的支持者，对中国AI持批评态度。但：

**数据是真的。**

- MiniMax M2.5 从 80.2% 降到 39.6% 是事实
- 中国模型在全新题目上普遍下滑是事实

### 需要思考的问题

1. **Benchmark 改革是必要的**
   - 静态测试确实容易被"刷题"
   - 动态测试才是真正实力的体现

2. **不要全盘否定中国 AI**
   - 批评过度拟合是对的
   - 但中国在某些领域（应用层、工程化）还是有优势的

3. **警惕"叙事"**
   - 80.2% vs 80.8% 的叙事很吸引人
   - 但如果没有真正的泛化能力，分数毫无意义

4. **长期看创新**
   - 真正的壁垒是**原创能力**，不是应试能力
   - 谁能在全新问题上持续创新，谁才是赢家

---

## 结语

**SWE-rebench 暴露了 AI 行业的"应试教育"问题。**

当分数不再能作弊，真实的差距就显露出来。

这不仅仅是关于谁更强，而是关于：
- 我们是否在构建**真正智能**的系统？
- 还是只是在构建**考试机器**？

答案将在未来几年揭晓。

---

*本文基于 @DavidOndrej1 的推文整理。*

*参考：SWE-rebench 测试结果，2026年2月*
